{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA_playground.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FpdS0hyUxf72"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMGYa0cMzxvHMHOWqrBa+J2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatJohaDH/LDA_playground/blob/main/LDA_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtyInHfbxDFD"
      },
      "source": [
        "\n",
        "#@title Welcome to Jupyter notebooks! { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown This _notebook_ is made to be run in google colab, which is a free\n",
        "#@markdown (though somewhat limited) cloud computing service offered by ... \n",
        "#@markdown Google.\n",
        "\n",
        "#@markdown A very brief summary: It lets us run python-backed calculations\n",
        "#@markdown in a webbrowser without having to install anything on our local\n",
        "#@markdown machines.\n",
        "\n",
        "#@markdown In order to operate this you need to be aware of a few things of how\n",
        "#@markdown it works. **Firstly**, this box (ending at the horizontal line below) is \n",
        "#@markdown called a *cell*. As a rule, each *cell* contains code. To *run* the\n",
        "#@markdown *cell*, i.e executing the instructions in the code, we can either\n",
        "#@markdown click the circle with a 'play' triangle in the cell's top-left corner.\n",
        "#@markdown Or we simply hold <shift> and press <enter> to run the selected cell.\n",
        "#@markdown **Secondly**, the instructions in subsequent cells often depend on\n",
        "#@markdown the successfull execution of preceeding cells, so try to execute them\n",
        "#@markdown in sequence. **Finally**, if you want to read the code in any of the\n",
        "#@markdown cells just double click on the text, and again to hide the code.\n",
        "\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact_manual\n",
        "import gensim\n",
        "import re\n",
        "import pandas as pd\n",
        "from nltk.stem.snowball import SnowballStemmer as stemmer\n",
        "\n",
        "#add the swedish stopwords too...\n",
        "stopwords = {'English': {_ for _ in gensim.parsing.preprocessing.STOPWORDS},\n",
        "             'Swedish': {_ for _ in pd.read_csv('https://raw.githubusercontent.com/peterdalle/svensktext/master/stoppord/stoppord.csv', header=0, encoding='utf-8')['word']}}\n",
        "\n",
        "stemmers = {'English': gensim.parsing.preprocessing.stem_text,\n",
        "            'Swedish': stemmer('swedish').stem}\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "from pprint import pprint\n",
        "\n",
        "#@markdown ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpdS0hyUxf72"
      },
      "source": [
        "# Topic Modelling Playground\n",
        "\n",
        "This notebook has been prepared with the intention to make the topic modelling process more transparent, by giving them easier access to some of the many knobs and dials of the process. If anything breaks, does not work as promised, is unclear or is missing do not hesitate to contact me and I will do my best to make things right: Mathias.Johansson@kom.lu.se\n",
        "\n",
        "## What is topic modelling?\n",
        "\n",
        "- Techniques for detecting _latent topics_ within a corpus\n",
        "- Threfore  a _topic_ does not refer to what we normally would consider a _topic_\n",
        "\n",
        "## What is it used for?\n",
        "\n",
        "- Document retreival\n",
        "- Literature review\n",
        "- Distant reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUFJzCO32Fir"
      },
      "source": [
        "# 1. Uploading corpus\n",
        "\n",
        "In order to run a Topic Model we need a corpus, so the first thing we need to do\n",
        "is to upload a single *.txt* file or a collection of *.txt* files encapsuled in\n",
        "a *.zip* file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHs23oOG2j5o"
      },
      "source": [
        "#@title Upload zip file { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown In order to have some data to play with, we need to upload it into \n",
        "#@markdown the running notebook. Running this cell (shift+enter) or pressing the\n",
        "#@markdown circle with an arrow in the top left corner of the cell will start the\n",
        "#@markdown file uploading widget.\n",
        "\n",
        "#@markdown The widget works similary to many other similar user-interactions,\n",
        "#@markdown namely, press \"Browse...\" and select the file(s) you want to upload.\n",
        "\n",
        "#@markdown Any type of file _can_ be uploaded, but the notebook is set to read\n",
        "#@markdown only *.txt* files and can unzip *.zip* files to access them if necessary.\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Gw3Enw2r6L"
      },
      "source": [
        "#@title Select zip to extract and use { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown Select the file you want to load your _corpus_ from and press\n",
        "#@markdown \"Run Interact\" to load it into memory.\n",
        "\n",
        "#@markdown **note:** You have to rerun the cell (shift+enter) to refresh the\n",
        "#@markdown list of available files.S\n",
        "\n",
        "sources = [_ for _ in os.listdir() if _.endswith('.txt') or _.endswith('.zip')]\n",
        "# source = widgets.Select(options=sources, description='Source:')\n",
        "# display(source)\n",
        "\n",
        "def load_files(source):\n",
        "    if source.endswith('.zip'):\n",
        "        if not os.path.exists(source[:-4]):\n",
        "            os.system(f'unzip {source}')\n",
        "        sources = [os.path.join(p,f) for p,d,fs in os.walk(source[:-4]) for f in fs if f.endswith('.txt')]\n",
        "        if len(sources) == 1:\n",
        "            return load_files(sources[0])\n",
        "        else:\n",
        "            texts = []\n",
        "            for fpath in sources:\n",
        "                with open(fpath, 'r') as f:\n",
        "                    texts.append(f.read())\n",
        "            df = pd.DataFrame({'source': sources, 'raw': texts})\n",
        "            return df\n",
        "    elif source.endswith('.txt'):\n",
        "        with open(source, 'r') as f:\n",
        "            lines = f.read()\n",
        "        sources = []\n",
        "        texts = [] \n",
        "        for nr, line in enumerate(re.split('\\n+', lines)):\n",
        "            sources.append(f'{source}_{nr}')\n",
        "            texts.append(line)\n",
        "        df = pd.DataFrame({'source': sources, 'raw': texts})\n",
        "        return df\n",
        "\n",
        "df = ''\n",
        "@interact_manual(source=widgets.Select(options=sources, description='Source:', rows=len(sources)+1),\n",
        "                 description='load')\n",
        "def set_df(source):\n",
        "    global df\n",
        "    df = load_files(source)\n",
        "    n_docs = len(df)\n",
        "    display(f'Loaded {n_docs} documents into corpus from {source}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaHNyYa2m2Vn"
      },
      "source": [
        "# 2. Preprocessing corpus\n",
        "Before we feed the corpus into the topic modelling algorithm we need to make the corpus redabla by the machine. There are many ways to do\n",
        "to do this in practice, but in essence they are all the same: Turning\n",
        "strings into vectors:\n",
        "\n",
        "$\\vec{d}=(0, 1, 4, 5, 0, 1, ..., 0)$\n",
        "\n",
        "To do this we are treated the texts as Bag of Words (BoW), we do not pay \n",
        "attention to sentence structure, we only count how many times each word\n",
        "(*token*) appears in each *document*.\n",
        "\n",
        "### The process in six steps:\n",
        "1. Turn all _documents_ to lowercase\n",
        "2. Strip _documents_ of puncuation, multiple whitespaces and numbers.\n",
        "3. Remove stopwords (and short words)\n",
        "4. Stem *tokens*\n",
        "5. Establish a vocabulary\n",
        "6. Count vocabulary *tokens* in each document \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHbbbXIF249r"
      },
      "source": [
        "#@title Pick a language { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown Since some of the preprocessing steps are language dependent we first\n",
        "#@markdown need to select which language to use. For this exersice there are two\n",
        "#@markdown options: English or Swedish.\n",
        "language = 'English' #@param['English', 'Swedish']\n",
        "gensim.parsing.preprocessing.DEFAULT_FILTERS[-1] = stemmers[language]\n",
        "\n",
        "print(f'You have selected: {language}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM5mU9Ju25YW"
      },
      "source": [
        "#@title Stopwords { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown __Stopwords__: words that are used too\n",
        "#@markdown often to convey any *real* meaning\n",
        "#@markdown on their own. For topic modelling\n",
        "#@markdown these terms can get in the way of\n",
        "#@markdown reaching relevant topics and are\n",
        "#@markdown therefore removed.\n",
        "\n",
        "#@markdown The simplest way to detect stopwords\n",
        "#@markdown is to use a preexisting list. Which is\n",
        "#@markdown exactly what we are doing here.\n",
        "\n",
        "show_stopwords = False #@param {type:\"boolean\"}\n",
        "\n",
        "add_stopwords = '' #@param {type: \"string\"}\n",
        "new_stopwords = {_ for _ in re.findall(r'\\w+', add_stopwords)}\n",
        "\n",
        "remove_stopwords = '' #@param {type: \"string\"}\n",
        "not_stopwords = {_ for _ in re.findall(r'\\w+', remove_stopwords)}\n",
        "\n",
        "gensim.parsing.preprocessing.STOPWORDS = (stopwords[language] | new_stopwords) - not_stopwords\n",
        "\n",
        "if show_stopwords:\n",
        "    pprint(gensim.parsing.preprocessing.STOPWORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YZqYJy125bI"
      },
      "source": [
        "#@title Preparing Vocabulary { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown The terms from the corpus that we end up using to vectorize the\n",
        "#@markdown documents is called *vocabulary*, *lexicon* or *dictionary*. I prefer\n",
        "#@markdown the term **vocabulary**.\n",
        "\n",
        "#@markdown Leave this box check if you want to ignore all the regular\n",
        "#@markdown preprocessing steps.\n",
        "use_raw = False #@param{type: \"boolean\"}\n",
        "\n",
        "#Preparing raw vocab\n",
        "if 'doc' not in df.columns or use_raw:\n",
        "    df['doc'] = df['raw'].apply(lambda raw: re.findall(r'[a-zA-ZååÅÄÖ\\d]+', raw))\n",
        "else:\n",
        "    df['doc'] = df['raw'].apply(gensim.parsing.preprocessing.preprocess_string)\n",
        "    old_shape = df.shape\n",
        "    # We also have to make sure that we remove the _documents_ that have no representation in this space.\n",
        "    df = df[df['doc'].apply(lambda doc: len(doc)>0)]\n",
        "    # print(f'New shape: {df.shape}\\nOld shape: {old_shap\n",
        "\n",
        "# Preparing the vocabulary\n",
        "vocab = gensim.corpora.Dictionary(documents=df['doc'])\n",
        "\n",
        "#@markdown Selecting the vocabulary by how many documents the terms appear in.\n",
        "#@markdown Everythign below the lower threshold, and above the upper threshold\n",
        "#@markdown will be removed from the *vocabulary*.\n",
        "\n",
        "@interact_manual(thresholds = widgets.IntRangeSlider(\n",
        "    value=[5,50], min=0, max=100, description='frequency (%)'\n",
        "))\n",
        "def vocab_prep(thresholds):\n",
        "    lower, upper = thresholds\n",
        "    global vocab\n",
        "    before = len(vocab)\n",
        "    if lower >0 or upper < 100:\n",
        "        lower /= 100\n",
        "        upper /= 100\n",
        "        vocab.filter_extremes(no_below=lower, no_above=upper)\n",
        "        vocab.compactify()\n",
        "        after = len(vocab)\n",
        "        print(f'Reduced the vocabulary from {before} terms to {after} terms.')\n",
        "    else:\n",
        "        print(f'The vocabulary has {before} terms.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9knFzthojg5R"
      },
      "source": [
        "#@title **Vectorizing the corpus** { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "#@markdown This is where the magic happens\n",
        "\n",
        "df['corpus'] = df['doc'].apply(vocab.doc2bow)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jPoysm7a6iD"
      },
      "source": [
        "# 3. Topic modelling\n",
        "\n",
        "Three important things to know about topic modelling:\n",
        "1. \"Topic\" refers to a *__latent topic__*, which is not what we normally mean when we use the term.\n",
        "2. Texts are considered compositions of **preexisting topics**\n",
        "3. The model will find exactly as many topics as we tell it to.\n",
        "\n",
        "The most common topic model is called **Latent Dirichlet Allocation (LDA)** and uses\n",
        "the dirichlet-distribution to assign probabilities to each topic-token pair.\n",
        "It is also the basis for other, more specialized, topic modelling algorithms:\n",
        "- Author Topic Modelling (ATM)\n",
        "- Structural Topic Modelling (STM) - Developed for surveys and adds metadata\n",
        "- Dynamic Topic modelling (DTM) - Adds a temporal element\n",
        "\n",
        "Since this process relies on a **random** initiation, different instances of the models, even when using the exact same corpus, will yield different topics. Similar, but not identical.\n",
        "\n",
        "In short the result of a LDA model is a large table that pairs each topic with a specific term in the *vocabulary*:\n",
        "\n",
        "| | topic 0 | topic 1 | ... | topic n |\n",
        "| -- | --- | --- | --- | --- |\n",
        "| word 0 | $p_{0, 0}$ | $p_{0, 1}$ | ... | $p_{0, n}$ |\n",
        "| word 1 | $p_{1, 0}$ | $p_{1, 1}$ | ... | $p_{1, n}$ |\n",
        "| ... | ... | ... | ... | ... |\n",
        "| word m | $p_{m, 0}$ | $p_{m, 1}$ | ... | $p_{m, n}$ |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNgr7dWt25dn"
      },
      "source": [
        "#@title Picking the number of topics { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "#@markdown Since the model will find exactly as many topics as we tell it to,\n",
        "#@markdown how do we select the **correct** number of topics?\n",
        "\n",
        "#@markdown There are essentially three approaches to this:\n",
        "#@markdown 1. Rely on your knowledge of the corpus, how many topics do you expect\n",
        "#@markdown to find?\n",
        "#@markdown 2. Pick a few, but disparate levels: 50, 250, 500 to use for distant\n",
        "#@markdown reading on different levels.\n",
        "#@markdown 3. Select a few metrics and calculate which is the best fit!\n",
        "\n",
        "#@markdown All of these are abitrary and which is more appropriate depends on:\n",
        "#@markdown - Familiarity with the corpus\n",
        "#@markdown - Size of the corpus\n",
        "#@markdown - Research question\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown If you are interested in the metric-based approach, run this cell to\n",
        "#@markdown get a graph of two such metrics calculated on the following number of topics:\n",
        "ntops = []\n",
        "umass = []\n",
        "logper = []\n",
        "topics = '3, 2, 5, 7, 11, 13, 17, 19, 23' #@param {type: \"string\"}\n",
        "topics = sorted(set(int(_) for _ in re.findall(r'\\d+', topics)))\n",
        "for ntop in topics:\n",
        "    ntops.append(ntop)\n",
        "    lda =gensim.models.LdaModel(corpus=df['corpus'], id2word=vocab, num_topics=ntop, alpha='auto', per_word_topics=True)\n",
        "    cm = gensim.models.coherencemodel.CoherenceModel(model=lda, corpus=df['corpus'], \n",
        "                                   dictionary=vocab, texts=df['doc'])\n",
        "    umass.append(cm.get_coherence())\n",
        "    # logper.append(lda.log_perplexity(df['corpus']))\n",
        "    logper.append(lda.bound(df['corpus']))\n",
        "fig, ax1 = plt.subplots()\n",
        "ax2 =ax1.twinx()\n",
        "\n",
        "p1, = ax1.plot(ntops, umass, 'b', label='U-mass - maximise')\n",
        "p2, =ax2.plot(ntops, logper, 'g--', label='Log perplexity - minimize')\n",
        "\n",
        "ax1.set_xticklabels(ntops)\n",
        "ax1.set_xticks(ntops)\n",
        "ax1.set_xlabel('Topics')\n",
        "ax1.set_ylabel('U-mass')\n",
        "ax2.set_ylabel('Log perplexity')\n",
        "\n",
        "\n",
        "ax1.legend(handles=[p1, p2])\n",
        "# topic_df.plot(x='topic', y=['umass'], kind='line')\n",
        "# topic_df.plot(x='topic', y=['logperplexity'], kind='line')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVgdxU_x25gN"
      },
      "source": [
        "#@title The topic model { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "#@markdown With a number of topics in mind we can finally fit a LDA-model to the\n",
        "#@markdown preprocessed corpus. Just fill in the number of topics and the\n",
        "\n",
        "#@markdown ###Selecting the number of topics\n",
        "number_of_topics =  19#@param {type: \"integer\", min: 2, max: 500}\n",
        "\n",
        "#@markdown ###A note on reproducability\n",
        "#@markdown In this instance an algorith called __Latent Dirichlet Allocation__ (LDA) is applied. Part of the initialisation of this algorithm is randomized, this means that we cannot reliably get identical results every time we run the algorithm. By assigning a value to the _seed_ we can make sure that every time __we__ run the same algorith with the the same data and same number of topics they will be identical.   \n",
        "seed = 5 #@param {type: \"number\"}\n",
        "\n",
        "lda = gensim.models.LdaModel(corpus=df['corpus'], id2word=vocab, num_topics=number_of_topics, alpha='auto', per_word_topics=True, random_state=seed)\n",
        "\n",
        "\n",
        "cm = gensim.models.CoherenceModel(coherence='c_v', texts=df['doc'], model=lda)\n",
        "\n",
        "top_topics = sorted([(c,i) for i,c in enumerate(cm.get_coherence_per_topic())], reverse=True)\n",
        "\n",
        "print(f'Calculation done with {number_of_topics} topics.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it06ttZu8neJ"
      },
      "source": [
        "# 4. Interpreting the results\n",
        "\n",
        "There are many ways to explore the resulting topics, and we will stick to the most basic approach:\n",
        "\n",
        "- Looking at the top terms of the topics.\n",
        "\n",
        "Note:\n",
        "- The topics are numbered **arbitrarily** starting from zero; they are no indication of which topic is *better*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPN7jn3L6B7f"
      },
      "source": [
        "#@title  Picking a specific topic { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "#@markdown To inspect the topic select a topic number and the number of terms you want to display. \n",
        "#@markdown Remember that the topic number has to be less than the number of topics you selected when running the model above.\n",
        "#@markdown It is also worth noting that it starts counting at _0_.\n",
        "topic_nr =  19#@param {type: \"integer\"}\n",
        "topic_nr = min(topic_nr, number_of_topics -1)\n",
        "number_of_words =  20 #@param {type : \"slider\", min: 1, max: 20}\n",
        "print(f'Topic: {topic_nr}')\n",
        "def print_topic_terms(topic_nr):\n",
        "    for term, p in lda.show_topic(topic_nr, number_of_words):\n",
        "        print(f'\\t{term} (p. {p:.4f})')\n",
        "print_topic_terms(topic_nr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CBubk69HHrg"
      },
      "source": [
        "#@title Inspecting top topics { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown Alternatively select how many of the __top__ topics and number of terms to inspect\n",
        "nr_top_topics =  20#@param {type: \"integer\"}\n",
        "\n",
        "number_of_words =  5 #@param {type : \"slider\", min: 1, max: 20}\n",
        "for coherence, topic in top_topics[:nr_top_topics]:\n",
        "    print(f'Topic: {topic} (coherence {coherence:.4f})')\n",
        "    print_topic_terms(topic)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}